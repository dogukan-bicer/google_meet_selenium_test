# Google Meet Selenium Test â€” Automated Video Conferencing Quality Assessment

**Repository:** [github.com/dogukan-bicer/google_meet_selenium_test](https://github.com/dogukan-bicer/google_meet_selenium_test)  
**Author:** DoÄŸukan BiÃ§er  
**Year:** 2025  
**Keywords:** Selenium Grid, WebRTC, Google Meet, Video Quality, Audio Quality, DINOv2, TRILLsson, Objective Assessment

---

## ğŸ§© Overview

This repository contains the implementation of an **automated test framework** designed to evaluate **video and audio quality** in Google Meet sessions using **Selenium Grid**.  
The project was developed as part of a **graduate thesis** focused on building an **objective, scalable, and repeatable** system for assessing video conferencing performance without relying solely on human evaluation.

Traditional manual tests often depend on subjective feedback or pixel-level metrics that fail to capture the true user experience.  
This system addresses that gap by integrating **deep learning-based image and audio similarity metrics** with **automated browser-based testing**.

---

## ğŸ¯ Motivation

During the COVID-19 pandemic, video conferencing platforms became essential for communication, education, and business.  
Local studies, such as that by **S. Ã–ztÃ¼rk et al. (2021)**, evaluated the functionality, performance, and usability of domestic WebRTC-based systems.  
However, those tests primarily relied on human perception and did not include **objective evaluations of image or sound quality**.

This project builds upon that foundation by focusing on **quantitative, machine-driven evaluation**.  
It bridges the gap between **subjective user experience** and **objective computational metrics**.

---

## ğŸš€ Project Goals

- Automate Google Meet testing using Selenium Grid nodes.  
- Capture and analyze **screen recordings** and **audio streams** in real time.  
- Evaluate video and audio quality using:
  - Classical metrics (e.g., **SSIM** â€” Structural Similarity Index Measure, **PSNR** â€” Peak Signal-to-Noise Ratio)
  - Deep learning-based metrics (**DINOv2**, **OpenCLIP**, **DISTS**, and **TRILLsson** for audio)
- Validate results on **NITE-IQA**, a human-rated image quality dataset.
- Develop a **reproducible and scalable** evaluation system for future WebRTC testing research.

---

## ğŸ§  Conceptual Background

### Video Quality Metrics
- **SSIM (Structural Similarity Index Measure)** â€” compares image luminance, contrast, and structure.  
- **PSNR (Peak Signal-to-Noise Ratio)** â€” measures pixel-wise similarity, less correlated with human perception.  
- **DINOv2** â€” extracts high-level semantic image features; captures perceptual differences like face or object distortions.  
- **OpenCLIP** â€” uses contrastive embeddings that align image and semantic meaning.  
- **DISTS (Deep Image Structure and Texture Similarity)** â€” measures both structural and textural consistency.

### Audio Quality Metric
- **TRILLsson (Distilled Universal Paralinguistic Speech Representations)** â€” captures perceptual and emotional properties of speech for sound comparison.

---

## âš™ï¸ System Architecture

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Selenium Hub â”‚
â”‚ (Test orchestration API) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Selenium Nodes â”‚ (multiple)
â”‚ (Chrome / Edge) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
Run Google Meet
Capture Streams
â”‚
â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
â”‚ FFmpeg â”‚ â†’ Records video/audio locally
â”‚ Recorder â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
â”‚
Post-Processing
(Frame extraction, metrics)
â”‚
â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
â”‚ Analysis â”‚ â†’ Compute SSIM, DINOv2, TRILLsson, etc.
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

yaml
Kodu kopyala

---

## ğŸ“¦ Repository Structure

.
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ docker/
â”‚ â””â”€â”€ docker-compose.yml # Optional: run Selenium Grid via Docker
â”œâ”€â”€ grid/
â”‚ â”œâ”€â”€ start_grid.sh # Starts Selenium Hub and Nodes
â”‚ â””â”€â”€ stop_grid.sh
â”œâ”€â”€ tests/
â”‚ â””â”€â”€ run_test.py # Main Google Meet test automation script
â”œâ”€â”€ recorders/
â”‚ â””â”€â”€ ffmpeg_record.sh # Capture screen and audio
â”œâ”€â”€ analysis/
â”‚ â”œâ”€â”€ extract_frames.py
â”‚ â”œâ”€â”€ compute_metrics.py
â”‚ â””â”€â”€ aggregate_results.py
â”œâ”€â”€ configs/
â”‚ â””â”€â”€ grid_config.json
â””â”€â”€ results/
â””â”€â”€ session01_metrics.csv

yaml
Kodu kopyala

---

## ğŸ§° Installation

### Requirements
- Python â‰¥ 3.8  
- Selenium â‰¥ 4.0  
- FFmpeg (for recording)  
- Chrome / Chromium + ChromeDriver  
- (Optional) Docker + Docker Compose  

### Setup
```bash
git clone https://github.com/dogukan-bicer/google_meet_selenium_test.git
cd google_meet_selenium_test

python -m venv .venv
source .venv/bin/activate   # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
Example requirements.txt:

nginx
Kodu kopyala
selenium
opencv-python
numpy
pandas
torch
tqdm
librosa
ffmpeg-python
scipy
ğŸ§ª Running Selenium Grid (via Docker)
Start the Selenium Hub and Chrome Node containers:

bash
Kodu kopyala
docker-compose -f docker/docker-compose.yml up -d
Access the Grid UI:
ğŸ‘‰ http://localhost:4444/ui/

Example docker/docker-compose.yml:

yaml
Kodu kopyala
version: "3"
services:
  selenium-hub:
    image: selenium/hub:4.8.0
    ports:
      - "4444:4444"

  chrome-node:
    image: selenium/node-chrome:4.8.0
    depends_on:
      - selenium-hub
    environment:
      - SE_EVENT_BUS_HOST=selenium-hub
      - SE_EVENT_BUS_PUBLISH_PORT=4442
      - SE_EVENT_BUS_SUBSCRIBE_PORT=4443
ğŸ§­ Running a Test Scenario
Example command:

bash
Kodu kopyala
python tests/run_test.py \
  --grid-url http://localhost:4444 \
  --browser chrome \
  --meeting-url "https://meet.google.com/your-meeting" \
  --record-dir ./recordings/session01
The script:

Connects to the Selenium Grid hub.

Launches Google Meet on remote nodes.

Performs actions (join meeting, mute/unmute, camera toggle).

Starts FFmpeg-based recording.

Saves video/audio streams locally for later analysis.

ğŸ¥ Recording Example (FFmpeg)
Example command used on each Node:

bash
Kodu kopyala
ffmpeg -y -video_size 1280x720 -f x11grab -i :0.0 \
-f pulse -i default -c:v libx264 -preset ultrafast \
-c:a aac recordings/session01/screen.mp4
ğŸ“Š Quality Analysis
After recording, compute visual/audio metrics:

bash
Kodu kopyala
python analysis/extract_frames.py --input ./recordings/session01
python analysis/compute_metrics.py \
  --frames ./recordings/session01/frames \
  --reference ./datasets/nite_iqa/reference \
  --metrics ssim,psnr,dinov2,dists \
  --out ./results/session01_metrics.csv
The script outputs a CSV file summarizing the similarity results per frame or clip.

ğŸ§® Metrics Used
Metric	Type	Description
SSIM	Classical	Structural Similarity Index Measure (structure, contrast, luminance)
PSNR	Classical	Peak Signal-to-Noise Ratio, derived from MSE
DINOv2	Deep	Self-supervised vision transformer embeddings
OpenCLIP	Deep	Image-text alignment features
DISTS	Deep	Deep Image Structure and Texture Similarity
TRILLsson	Deep	Audio embedding capturing paralinguistic cues

ğŸ“š Dataset Evaluation
All metrics were validated against the NITE-IQA dataset â€” a human-rated image quality benchmark.
Each metricâ€™s output was compared to Mean Opinion Scores (MOS) to assess correlation with human perception.

This approach allows:

Objective evaluation of deep vs. classical methods.

Insight into which metric better represents perceptual video degradation.

ğŸ§¾ Reproducing Thesis Experiments
To replicate results from the thesis:

bash
Kodu kopyala
bash grid/start_grid.sh
python orchestrate/run_experiments.py --config configs/experiment.yml
python analysis/aggregate_results.py --input results/ --out final_report.csv
Outputs include:

Metric values per scenario

Correlation scores (Pearson, Spearman)

Final report table summarizing results

ğŸ§  Key Contributions
Developed a fully automated framework for video conferencing quality testing.

Combined Selenium Grid, FFmpeg, and deep learning-based metrics in a single system.

Demonstrated how objective quality assessment can replace or complement human evaluations.

Conducted extensive experiments on NITE-IQA and real-world Google Meet recordings.

Provided an open, extensible foundation for future WebRTC and multimedia testing research.

ğŸ“„ License
vbnet
Kodu kopyala
MIT License
Â© 2025 DoÄŸukan BiÃ§er
ğŸ”— Citation
If you use this code, please cite:

D. BiÃ§er, â€œObjective Quality Assessment of Video Conferencing Systems Using Deep Learning-Based Metrics,â€ Masterâ€™s Thesis, 2025.

ğŸ“¬ Contact
For questions or collaboration:
ğŸ“§ github.com/dogukan-bicer

ğŸŒŸ Acknowledgements
This work was inspired by S. Ã–ztÃ¼rk et al. (2021) â€” â€œFunctionality, Performance and Usability Tests of WebRTC-Based Video Conferencing Products,â€ which laid the foundation for domestic video conferencing test automation efforts during the COVID-19 pandemic.
